{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aciNJUpr07d9"
      },
      "outputs": [],
      "source": [
        "# Pip install scikeras and python-Levenshtein, uncomment them and restart the\n",
        "# session, pip install fuzzywuzzy, uncomment and restart, and then\n",
        "# everything should be able to run\n",
        "\n",
        "import numpy as np\n",
        "import random, time, sys\n",
        "\n",
        "# data processing\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "#!pip install --upgrade tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import save_model, load_model\n",
        "\n",
        "# evaluation and hyperparameter tuning\n",
        "#!pip install scikeras[tensorflow]\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# word processing\n",
        "#!pip install python-Levenshtein\n",
        "#!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This only became an issue later on in the project when importing modules\n",
        "# for Hyperparameter tuning\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QkXqwRiacvm",
        "outputId": "a1b3cc17-2696-4da5-9416-0d6c42ddfdfa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.1\n",
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the JSON file\n",
        "with open('intents.json', 'r') as file:\n",
        "    intents = json.load(file)"
      ],
      "metadata": {
        "id": "BwPHe_Jg-WF4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf7VWHOw-cQ3",
        "outputId": "4be3e487-41ac-46c9-b08e-de00ad75c1d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'greeting',\n",
              "   'patterns': ['Hi there',\n",
              "    'How are you',\n",
              "    'Is anyone there?',\n",
              "    'Hey',\n",
              "    'Hola',\n",
              "    'Hello',\n",
              "    'Good day'],\n",
              "   'responses': ['Hello, thanks for asking',\n",
              "    'Good to see you again',\n",
              "    'Hi there, how can I help?'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['Bye',\n",
              "    'See you later',\n",
              "    'Goodbye',\n",
              "    'Nice chatting to you, bye',\n",
              "    'Till next time'],\n",
              "   'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['Thanks',\n",
              "    'Thank you',\n",
              "    \"That's helpful\",\n",
              "    'Awesome, thanks',\n",
              "    'Thanks for helping me'],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My pleasure'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'noanswer',\n",
              "   'patterns': [],\n",
              "   'responses': [\"Sorry, can't understand you\",\n",
              "    'Please give me more info',\n",
              "    'Not sure I understand'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'options',\n",
              "   'patterns': ['How you could help me?',\n",
              "    'What you can do?',\n",
              "    'What help you provide?',\n",
              "    'How you can be helpful?',\n",
              "    'What support is offered'],\n",
              "   'responses': ['I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies',\n",
              "    'Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'adverse_drug',\n",
              "   'patterns': ['How to check Adverse drug reaction?',\n",
              "    'Open adverse drugs module',\n",
              "    'Give me a list of drugs causing adverse behavior',\n",
              "    'List all drugs suitable for patient with adverse reaction',\n",
              "    'Which drugs dont have adverse reaction?'],\n",
              "   'responses': ['Navigating to Adverse drug reaction module'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'blood_pressure',\n",
              "   'patterns': ['Open blood pressure module',\n",
              "    'Task related to blood pressure',\n",
              "    'Blood pressure data entry',\n",
              "    'I want to log blood pressure results',\n",
              "    'Blood pressure data management'],\n",
              "   'responses': ['Navigating to Blood Pressure module'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'blood_pressure_search',\n",
              "   'patterns': ['I want to search for blood pressure result history',\n",
              "    'Blood pressure for patient',\n",
              "    'Load patient blood pressure result',\n",
              "    'Show blood pressure results for patient',\n",
              "    'Find blood pressure results by ID'],\n",
              "   'responses': ['Please provide Patient ID', 'Patient ID?'],\n",
              "   'context': ['search_blood_pressure_by_patient_id']},\n",
              "  {'tag': 'search_blood_pressure_by_patient_id',\n",
              "   'patterns': [],\n",
              "   'responses': ['Loading Blood pressure result for Patient'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'pharmacy_search',\n",
              "   'patterns': ['Find me a pharmacy',\n",
              "    'Find pharmacy',\n",
              "    'List of pharmacies nearby',\n",
              "    'Locate pharmacy',\n",
              "    'Search pharmacy'],\n",
              "   'responses': ['Please provide pharmacy name'],\n",
              "   'context': ['search_pharmacy_by_name']},\n",
              "  {'tag': 'search_pharmacy_by_name',\n",
              "   'patterns': [],\n",
              "   'responses': ['Loading pharmacy details'],\n",
              "   'context': ['']},\n",
              "  {'tag': 'hospital_search',\n",
              "   'patterns': ['Lookup for hospital',\n",
              "    'Searching for hospital to transfer patient',\n",
              "    'I want to search hospital data',\n",
              "    'Hospital lookup for patient',\n",
              "    'Looking up hospital details'],\n",
              "   'responses': ['Please provide hospital name or location'],\n",
              "   'context': ['search_hospital_by_params']},\n",
              "  {'tag': 'search_hospital_by_params',\n",
              "   'patterns': [],\n",
              "   'responses': ['Please provide hospital type'],\n",
              "   'context': ['search_hospital_by_type']},\n",
              "  {'tag': 'search_hospital_by_type',\n",
              "   'patterns': [],\n",
              "   'responses': ['Loading hospital details'],\n",
              "   'context': ['']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in intents['intents']:\n",
        "  for pattern in tag['patterns']:\n",
        "    print(f\"{pattern.lower()}\\t{tag['tag']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnirIeIwPL5e",
        "outputId": "c18a5470-6ad6-4afc-83bd-93b7e7b8e4b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi there\tgreeting\n",
            "how are you\tgreeting\n",
            "is anyone there?\tgreeting\n",
            "hey\tgreeting\n",
            "hola\tgreeting\n",
            "hello\tgreeting\n",
            "good day\tgreeting\n",
            "bye\tgoodbye\n",
            "see you later\tgoodbye\n",
            "goodbye\tgoodbye\n",
            "nice chatting to you, bye\tgoodbye\n",
            "till next time\tgoodbye\n",
            "thanks\tthanks\n",
            "thank you\tthanks\n",
            "that's helpful\tthanks\n",
            "awesome, thanks\tthanks\n",
            "thanks for helping me\tthanks\n",
            "how you could help me?\toptions\n",
            "what you can do?\toptions\n",
            "what help you provide?\toptions\n",
            "how you can be helpful?\toptions\n",
            "what support is offered\toptions\n",
            "how to check adverse drug reaction?\tadverse_drug\n",
            "open adverse drugs module\tadverse_drug\n",
            "give me a list of drugs causing adverse behavior\tadverse_drug\n",
            "list all drugs suitable for patient with adverse reaction\tadverse_drug\n",
            "which drugs dont have adverse reaction?\tadverse_drug\n",
            "open blood pressure module\tblood_pressure\n",
            "task related to blood pressure\tblood_pressure\n",
            "blood pressure data entry\tblood_pressure\n",
            "i want to log blood pressure results\tblood_pressure\n",
            "blood pressure data management\tblood_pressure\n",
            "i want to search for blood pressure result history\tblood_pressure_search\n",
            "blood pressure for patient\tblood_pressure_search\n",
            "load patient blood pressure result\tblood_pressure_search\n",
            "show blood pressure results for patient\tblood_pressure_search\n",
            "find blood pressure results by id\tblood_pressure_search\n",
            "find me a pharmacy\tpharmacy_search\n",
            "find pharmacy\tpharmacy_search\n",
            "list of pharmacies nearby\tpharmacy_search\n",
            "locate pharmacy\tpharmacy_search\n",
            "search pharmacy\tpharmacy_search\n",
            "lookup for hospital\thospital_search\n",
            "searching for hospital to transfer patient\thospital_search\n",
            "i want to search hospital data\thospital_search\n",
            "hospital lookup for patient\thospital_search\n",
            "looking up hospital details\thospital_search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our training data\n",
        "# We use append mode with the txt in order to create a txt file with this\n",
        "# name in our working directory\n",
        "# Since some of our patterns include a comma, we can't use comma as our\n",
        "# delimiter and we will use a tab \"\\t\" instead\n",
        "with open('training_data_hospitalBot.txt', \"a\") as f:\n",
        "    f.write(\"patterns\\ttags\\n\")\n",
        "    for tag in intents['intents']:\n",
        "      for pattern in tag['patterns']:\n",
        "        f.write(f\"{pattern.lower()}\\t{tag['tag']}\\n\")"
      ],
      "metadata": {
        "id": "bv9posLMVptA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = pd.read_csv(\"/content/training_data_hospitalBot.txt\",\n",
        "                            delimiter='\\t')\n",
        "training_data.drop(index=training_data.index[0], axis=0, inplace=True) # Drop\n",
        "# the first row which mimics the headings\n",
        "training_data = training_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "8_TQ_hg9YtZi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2wapRPO3ZejQ",
        "outputId": "45614bdc-3322-4d24-a1c1-91767ad035cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       patterns             tags\n",
              "0                                   how are you         greeting\n",
              "1                              is anyone there?         greeting\n",
              "2                                           hey         greeting\n",
              "3                                          hola         greeting\n",
              "4                                         hello         greeting\n",
              "..                                          ...              ...\n",
              "185                         lookup for hospital  hospital_search\n",
              "186  searching for hospital to transfer patient  hospital_search\n",
              "187              i want to search hospital data  hospital_search\n",
              "188                 hospital lookup for patient  hospital_search\n",
              "189                 looking up hospital details  hospital_search\n",
              "\n",
              "[190 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4a32844-5a07-4df8-87a4-dfc1bf5ce879\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patterns</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>how are you</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is anyone there?</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hey</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hola</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hello</td>\n",
              "      <td>greeting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>lookup for hospital</td>\n",
              "      <td>hospital_search</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>searching for hospital to transfer patient</td>\n",
              "      <td>hospital_search</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>i want to search hospital data</td>\n",
              "      <td>hospital_search</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>hospital lookup for patient</td>\n",
              "      <td>hospital_search</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>looking up hospital details</td>\n",
              "      <td>hospital_search</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>190 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4a32844-5a07-4df8-87a4-dfc1bf5ce879')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f4a32844-5a07-4df8-87a4-dfc1bf5ce879 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f4a32844-5a07-4df8-87a4-dfc1bf5ce879');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9dfaa17a-6c0d-4fd2-ad46-4b9e738cb31e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9dfaa17a-6c0d-4fd2-ad46-4b9e738cb31e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9dfaa17a-6c0d-4fd2-ad46-4b9e738cb31e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing training data; transforming it into something that our\n",
        "# computers can understand\n",
        "# convert to lowercase so that the bot isn’t distinguishing the case of\n",
        "# the characters\n",
        "training_data[\"patterns\"] = training_data[\"patterns\"].str.lower()\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
        "training_data_tfidf = vectorizer.fit_transform(training_data[\"patterns\"]).toarray()"
      ],
      "metadata": {
        "id": "3B-Nd4-9DmEA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_tfidf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n54ae_zHe-Mk",
        "outputId": "39e36117-d87b-4ab0-bfc7-7460aaf3b66e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.35056373, 0.        ,\n",
              "        0.38552431],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing target variable (tags); this is also used to transform\n",
        "# categories, in our case our tags, into numerical format\n",
        "le = LabelEncoder()\n",
        "training_data_tags_le = pd.DataFrame({\"tags\": le.fit_transform(training_data[\"tags\"])})\n",
        "# And additionally we use Dummy Encoding so that there won’t be any intrinsic\n",
        "# order or priority within the categories. Each category will be treated as an\n",
        "# independent and equally important feature.\n",
        "training_data_tags_dummy_encoded = pd.get_dummies(training_data_tags_le[\"tags\"]).to_numpy()"
      ],
      "metadata": {
        "id": "PuiO6PDPfCMi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the DNN\n",
        "hospitalbot = Sequential()\n",
        "# The input layer will have 10 nodes and the shape of our training data will\n",
        "# determine how the input data is entering the neural network\n",
        "# Since we don’t explicitely type anything, ReLU is the activation function\n",
        "# we use in the intput layer and our three hidden layers, it’s the default\n",
        "# function.\n",
        "hospitalbot.add(Dense(10, input_shape=(len(training_data_tfidf[0]),)))\n",
        "hospitalbot.add(Dense(8))\n",
        "hospitalbot.add(Dense(8))\n",
        "hospitalbot.add(Dense(6))\n",
        "# And we use softmax as our activation function in the Output layer since\n",
        "# it’s a multi-class classification problem\n",
        "hospitalbot.add(Dense(len(training_data_tags_dummy_encoded[0]),\n",
        "                      activation=\"softmax\"))\n",
        "hospitalbot.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "                    metrics=\"accuracy\")"
      ],
      "metadata": {
        "id": "Xh7Di7q7Qt2F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting DNN\n",
        "hospitalbot.fit(training_data_tfidf, training_data_tags_dummy_encoded,\n",
        "                epochs=100, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK5KHjNnij4M",
        "outputId": "da473c4e-b0c1-4598-e0db-fe7297a65a4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "6/6 [==============================] - 2s 10ms/step - loss: 2.2966 - accuracy: 0.1421\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 2.2496 - accuracy: 0.2263\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2.2120 - accuracy: 0.2737\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.1740 - accuracy: 0.3053\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 2.1322 - accuracy: 0.3263\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 2.0880 - accuracy: 0.3474\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 18ms/step - loss: 2.0403 - accuracy: 0.3632\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 1.9893 - accuracy: 0.4053\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 1.9343 - accuracy: 0.4211\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.8771 - accuracy: 0.4316\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.8165 - accuracy: 0.4316\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 1.7568 - accuracy: 0.4632\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1.6970 - accuracy: 0.4632\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 1.6377 - accuracy: 0.4632\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.5792 - accuracy: 0.4684\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 1.5229 - accuracy: 0.4947\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 22ms/step - loss: 1.4663 - accuracy: 0.5053\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 23ms/step - loss: 1.4134 - accuracy: 0.5000\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 1.3592 - accuracy: 0.5053\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 1.3092 - accuracy: 0.5211\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.2585 - accuracy: 0.5526\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.2113 - accuracy: 0.5842\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 1.1642 - accuracy: 0.5947\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1.1208 - accuracy: 0.6053\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 1.0772 - accuracy: 0.6474\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 1.0341 - accuracy: 0.6684\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.9950 - accuracy: 0.6947\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.9546 - accuracy: 0.6947\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.9171 - accuracy: 0.7105\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.8801 - accuracy: 0.7105\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.8438 - accuracy: 0.7105\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.8103 - accuracy: 0.7316\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.7762 - accuracy: 0.7737\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.7428 - accuracy: 0.7737\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.7109 - accuracy: 0.7737\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.8263\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.6504 - accuracy: 0.8421\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.6220 - accuracy: 0.8947\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.5940 - accuracy: 0.9158\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5672 - accuracy: 0.9263\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5388 - accuracy: 0.9368\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.5128 - accuracy: 0.9421\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4876 - accuracy: 0.9421\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.4630 - accuracy: 0.9421\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4377 - accuracy: 0.9421\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4150 - accuracy: 0.9421\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.3909 - accuracy: 0.9421\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.3693 - accuracy: 0.9421\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3480 - accuracy: 0.9421\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 0.3260 - accuracy: 0.9421\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3067 - accuracy: 0.9421\n",
            "Epoch 52/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.2880 - accuracy: 0.9421\n",
            "Epoch 53/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2710 - accuracy: 0.9421\n",
            "Epoch 54/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2552 - accuracy: 0.9421\n",
            "Epoch 55/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.2406 - accuracy: 0.9421\n",
            "Epoch 56/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2259 - accuracy: 0.9421\n",
            "Epoch 57/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2127 - accuracy: 0.9421\n",
            "Epoch 58/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2020 - accuracy: 0.9421\n",
            "Epoch 59/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1902 - accuracy: 0.9421\n",
            "Epoch 60/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1816 - accuracy: 0.9421\n",
            "Epoch 61/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1724 - accuracy: 0.9421\n",
            "Epoch 62/100\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.1644 - accuracy: 0.9421\n",
            "Epoch 63/100\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1557 - accuracy: 0.9421\n",
            "Epoch 64/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1490 - accuracy: 0.9421\n",
            "Epoch 65/100\n",
            "6/6 [==============================] - 0s 17ms/step - loss: 0.1439 - accuracy: 0.9421\n",
            "Epoch 66/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1381 - accuracy: 0.9421\n",
            "Epoch 67/100\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1328 - accuracy: 0.9421\n",
            "Epoch 68/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1276 - accuracy: 0.9579\n",
            "Epoch 69/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1241 - accuracy: 0.9579\n",
            "Epoch 70/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1203 - accuracy: 0.9579\n",
            "Epoch 71/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9579\n",
            "Epoch 72/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1129 - accuracy: 0.9474\n",
            "Epoch 73/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1110 - accuracy: 0.9579\n",
            "Epoch 74/100\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1066 - accuracy: 0.9579\n",
            "Epoch 75/100\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1034 - accuracy: 0.9579\n",
            "Epoch 76/100\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1017 - accuracy: 0.9579\n",
            "Epoch 77/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0995 - accuracy: 0.9579\n",
            "Epoch 78/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0975 - accuracy: 0.9579\n",
            "Epoch 79/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0975 - accuracy: 0.9579\n",
            "Epoch 80/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0934 - accuracy: 0.9579\n",
            "Epoch 81/100\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9526\n",
            "Epoch 82/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0913 - accuracy: 0.9579\n",
            "Epoch 83/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9579\n",
            "Epoch 84/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0877 - accuracy: 0.9526\n",
            "Epoch 85/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0886 - accuracy: 0.9579\n",
            "Epoch 86/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9579\n",
            "Epoch 87/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0862 - accuracy: 0.9579\n",
            "Epoch 88/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0841 - accuracy: 0.9579\n",
            "Epoch 89/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0848 - accuracy: 0.9474\n",
            "Epoch 90/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0834 - accuracy: 0.9526\n",
            "Epoch 91/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0828 - accuracy: 0.9579\n",
            "Epoch 92/100\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0846 - accuracy: 0.9579\n",
            "Epoch 93/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0814 - accuracy: 0.9579\n",
            "Epoch 94/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9526\n",
            "Epoch 95/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0811 - accuracy: 0.9579\n",
            "Epoch 96/100\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0796 - accuracy: 0.9579\n",
            "Epoch 97/100\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0790 - accuracy: 0.9579\n",
            "Epoch 98/100\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.9474\n",
            "Epoch 99/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9526\n",
            "Epoch 100/100\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 0.0792 - accuracy: 0.9579\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79a3d42d2a40>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(hospitalbot, \"HospitalBot_v1\")"
      ],
      "metadata": {
        "id": "SP28MjETiyvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af1dcf9-07df-4649-efe1-f1206d83a15b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = load_model(\"HospitalBot_v1\")"
      ],
      "metadata": {
        "id": "8nKvsPQsj3Xo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = [item['tag'] for item in intents['intents']]\n",
        "tags"
      ],
      "metadata": {
        "id": "lXN_41kUkI8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa97dd10-4a20-4303-d445-f705e9558139"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['greeting',\n",
              " 'goodbye',\n",
              " 'thanks',\n",
              " 'noanswer',\n",
              " 'options',\n",
              " 'adverse_drug',\n",
              " 'blood_pressure',\n",
              " 'blood_pressure_search',\n",
              " 'search_blood_pressure_by_patient_id',\n",
              " 'pharmacy_search',\n",
              " 'search_pharmacy_by_name',\n",
              " 'hospital_search',\n",
              " 'search_hospital_by_params',\n",
              " 'search_hospital_by_type']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp = [item['responses'] for item in intents['intents']]\n",
        "resp"
      ],
      "metadata": {
        "id": "K3mGNQGflpUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4c4b58-98bd-484f-902a-e5f9d639b1a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hello, thanks for asking',\n",
              "  'Good to see you again',\n",
              "  'Hi there, how can I help?'],\n",
              " ['See you!', 'Have a nice day', 'Bye! Come back again soon.'],\n",
              " ['Happy to help!', 'Any time!', 'My pleasure'],\n",
              " [\"Sorry, can't understand you\",\n",
              "  'Please give me more info',\n",
              "  'Not sure I understand'],\n",
              " ['I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies',\n",
              "  'Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies'],\n",
              " ['Navigating to Adverse drug reaction module'],\n",
              " ['Navigating to Blood Pressure module'],\n",
              " ['Please provide Patient ID', 'Patient ID?'],\n",
              " ['Loading Blood pressure result for Patient'],\n",
              " ['Please provide pharmacy name'],\n",
              " ['Loading pharmacy details'],\n",
              " ['Please provide hospital name or location'],\n",
              " ['Please provide hospital type'],\n",
              " ['Loading hospital details']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_responses = dict(zip(tags, resp))\n",
        "tag_responses"
      ],
      "metadata": {
        "id": "nPkpC02olX9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f6ae5b-f905-48ad-a19f-744df768b458"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'greeting': ['Hello, thanks for asking',\n",
              "  'Good to see you again',\n",
              "  'Hi there, how can I help?'],\n",
              " 'goodbye': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'],\n",
              " 'thanks': ['Happy to help!', 'Any time!', 'My pleasure'],\n",
              " 'noanswer': [\"Sorry, can't understand you\",\n",
              "  'Please give me more info',\n",
              "  'Not sure I understand'],\n",
              " 'options': ['I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies',\n",
              "  'Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies'],\n",
              " 'adverse_drug': ['Navigating to Adverse drug reaction module'],\n",
              " 'blood_pressure': ['Navigating to Blood Pressure module'],\n",
              " 'blood_pressure_search': ['Please provide Patient ID', 'Patient ID?'],\n",
              " 'search_blood_pressure_by_patient_id': ['Loading Blood pressure result for Patient'],\n",
              " 'pharmacy_search': ['Please provide pharmacy name'],\n",
              " 'search_pharmacy_by_name': ['Loading pharmacy details'],\n",
              " 'hospital_search': ['Please provide hospital name or location'],\n",
              " 'search_hospital_by_params': ['Please provide hospital type'],\n",
              " 'search_hospital_by_type': ['Loading hospital details']}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# json_object = json.dumps(tag_responses, indent=4)\n",
        "\n",
        "# with open(\"responses.json\", \"w\") as outfile:\n",
        "#   outfile.write(json_object)"
      ],
      "metadata": {
        "id": "Ld_6B8KqmkXs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming input and predicting intent\n",
        "def predict_tag(user_input):\n",
        "  user_input_tfidf = vectorizer.transform([user_input.lower()]).toarray()\n",
        "  predicted_proba = hospitalbot.predict(user_input_tfidf)\n",
        "  encoded_label = [np.argmax(predicted_proba)]\n",
        "  predicted_tag = le.inverse_transform(encoded_label)[0]\n",
        "  return predicted_tag"
      ],
      "metadata": {
        "id": "wgfR-s6CnnSt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating our chat loop\n",
        "def start_chat():\n",
        "  print(\"---------------HospitalBot V1---------------\")\n",
        "  print(\"Ask any queries!\")\n",
        "  print(\"Type EXIT to quit\\n\")\n",
        "  while True:\n",
        "    user_input = input(\"Ask anything: \")\n",
        "    if user_input == \"EXIT\":\n",
        "      time.sleep(1)\n",
        "      break\n",
        "    else:\n",
        "      if user_input:\n",
        "        tag = predict_tag(user_input)\n",
        "        response = random.choice(tag_responses[tag])\n",
        "        time.sleep(1)\n",
        "        print(response)\n",
        "      else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "kddqQHf7oWp4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_chat(\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "7BBEw4_tpQfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9f1491-0554-476f-cc52-ba6d237ea6c2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------HospitalBot V1---------------\n",
            "Ask any queries!\n",
            "Type EXIT to quit\n",
            "\n",
            "Ask anything: Hello\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Hello, thanks for asking\n",
            "Ask anything: sldbnsdlb\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Good to see you again\n",
            "Ask anything: pharmacy\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Please provide pharmacy name\n",
            "Ask anything: Apoteket\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Hello, thanks for asking\n",
            "Ask anything: EXIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is not the smartest tool in the shed haha. Let's check accuracy!"
      ],
      "metadata": {
        "id": "HuwHYCrXZbj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    training_data_tfidf,\n",
        "    training_data_tags_dummy_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Defining early stopping callback to prevent overfitting by stopping the\n",
        "# training when the model's performance on a validation set stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3,\n",
        "                               restore_best_weights=True)\n",
        "\n",
        "# Training the model with early stopping implemented\n",
        "history = hospitalbot.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,  # Increase the number of epochs or set it based on your training needs\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    validation_split=0.1,  # Use a portion of the training data for validation\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "BYjYw6gXZaMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be542961-b6d7-42d9-9948-85680231c816"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.0756 - accuracy: 0.9559 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.0728 - accuracy: 0.9706 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0720 - accuracy: 0.9706 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0718 - accuracy: 0.9706 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0711 - accuracy: 0.9706 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0699 - accuracy: 0.9706 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0721 - accuracy: 0.9706 - val_loss: 0.0020 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "test_metrics = hospitalbot.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Displaying the test accuracy and other metrics\n",
        "print(f\"Test Accuracy: {test_metrics[1]*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_metrics[0]}\")\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = hospitalbot.predict(X_test)\n",
        "\n",
        "# Converting predictions to class labels\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_test_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Displaying classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))"
      ],
      "metadata": {
        "id": "-XDLV97Ub-ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6f9c2b-21a6-4d30-8e6e-14106d3db312"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 89.47%\n",
            "Test Loss: 0.15313132107257843\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         4\n",
            "           1       1.00      1.00      1.00         5\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       1.00      1.00      1.00         3\n",
            "           4       0.80      1.00      0.89         4\n",
            "           5       1.00      1.00      1.00         3\n",
            "           6       1.00      0.60      0.75        10\n",
            "           7       1.00      1.00      1.00         3\n",
            "           9       0.57      1.00      0.73         4\n",
            "\n",
            "    accuracy                           0.89        38\n",
            "   macro avg       0.93      0.96      0.93        38\n",
            "weighted avg       0.93      0.89      0.89        38\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is clearly overfitted and has memorized every single response in the JSON file (but somehow still gets it completely wrong sometimes). We want to achieve a balance between training accuracy and generalization"
      ],
      "metadata": {
        "id": "c5imE9ubcuV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's start on HospitalBot V2!<br>\n",
        "\n",
        "Apart from improving our model with hyperparameter tuning, we're going to implement the following:\n",
        "* Context Handling: Utilizing the \"context\" parameter from the original JSON file to get context-aware responses or prompting for user input\n",
        "* User Experience and implementing response time based on context\n",
        "* User Input Preprocessing in the form of:\n",
        "  * Intent Confidence Threshold: We need to pass a threshold to confidently say “okay, that connects to this tag”. This will almost inevitably be a trade-off between making confident predictions and avoiding incorrect predictions\n",
        "  * Handling Unknown Inputs and Error Handling: We will introduce default \"I don't understand messages\" if we don't pass the confidence threshold\n",
        "  * Fuzzy Matching: The bot will be able to understand slight variations and misspellings of words and phrases\n"
      ],
      "metadata": {
        "id": "vIbeabdrBAzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clarification both for readers and myself on Context Handling: If we write something within the tag of 'blood_pressure_search', the bot will provide a response of either 'Please provide Patient ID?' or simply 'Patient ID?'. But after we have entered that (I'm also now seeing, it shouldn't say \"Ask anything\" when we enter that as well), it won't go back to its original \"Hi there, how can I help?\", but rather it will pick the context from the tag \"blood_pressure_search\", i.e. the current context will be \"search_blood_pressure_by_patient_id\" and so that will be the new tag and it won't even ask for input but instead give the response in that tag \"Loading Blood pressure result for Patient\". And from there, it can say \"Good luck! I will be here if you need me again\" and then shut down or something.<br>\n",
        "The other key point to this, that just clicked for me, is that we don't predict a tag based on user input if we have a current context!"
      ],
      "metadata": {
        "id": "gs1L4YpDzp-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning"
      ],
      "metadata": {
        "id": "jAK4jfMSGyNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment to perform it again. It's commented out to have the entire program fun faster"
      ],
      "metadata": {
        "id": "eG-S-AeGVM26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Defining a simple neural network model based on our previous one\n",
        "# def create_model(optimizer='rmsprop', activation='relu', learning_rate=0.001):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(10, input_shape=(len(training_data_tfidf[0]),),\n",
        "#                     activation=activation))\n",
        "#     model.add(Dense(8, activation=activation))\n",
        "#     model.add(Dense(8, activation=activation))\n",
        "#     model.add(Dense(6, activation=activation))\n",
        "#     model.add(Dense(len(training_data_tags_dummy_encoded[0]),\n",
        "#                     activation=\"softmax\"))\n",
        "\n",
        "#     # Use the specified optimizer and learning rate\n",
        "#     if optimizer == 'adam':\n",
        "#         opt = Adam(learning_rate=learning_rate)\n",
        "#     elif optimizer == 'sgd':\n",
        "#         opt = SGD(learning_rate=learning_rate)\n",
        "#     else:\n",
        "#         opt = optimizer  # Use the default optimizer if specified\n",
        "\n",
        "#     model.compile(optimizer=opt, loss=\"categorical_crossentropy\",\n",
        "#                   metrics=[\"accuracy\"])\n",
        "#     return model\n",
        "\n",
        "# # Wrap the Keras model as an estimator; KerasClassifier is a convenient\n",
        "# wrapper allowing us to use a Keras neural network model as if it was a\n",
        "# classifier in the Scikit-Learn framework. The RandomizedSearchCV kinda\n",
        "# needs a familiar estimator to use its functions and so we use the\n",
        "# KerasClassifier to kinda mimic that behavior; to use our Keras model within\n",
        "# this Scikit-Learn context\n",
        "# estimator = KerasClassifier(build_fn=create_model, activation='relu',\n",
        "#                             epochs=10, batch_size=32, learning_rate=0.001,\n",
        "#                             verbose=0)\n",
        "\n",
        "\n",
        "# # Define hyperparameters and their distributions for RandomizedSearchCV\n",
        "# param_dist = {\n",
        "#     'optimizer': [Adam(), SGD()],\n",
        "#     'activation': ['relu', 'sigmoid'],\n",
        "#     'batch_size': [16, 32, 64],\n",
        "#     'epochs': [5, 10, 15],\n",
        "#     'learning_rate': uniform(loc=0.0001, scale=0.1),\n",
        "# }\n",
        "\n",
        "\n",
        "# # Perform random search\n",
        "# random_search = RandomizedSearchCV(estimator=estimator,\n",
        "#                                    param_distributions=param_dist,\n",
        "#                                    scoring='accuracy', cv=3, n_iter=10)\n",
        "# random_search_result = random_search.fit(X_train, y_train)\n",
        "\n",
        "# # Print the best hyperparameters\n",
        "# print(\"Best: %f using %s\" % (random_search_result.best_score_,\n",
        "#                              random_search_result.best_params_))"
      ],
      "metadata": {
        "id": "F0-UIbGOt5fE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a DNN with our best parameters!\n",
        "best_params = {'activation': 'relu',\n",
        "               'batch_size': 16,\n",
        "               'epochs': 15,\n",
        "               'learning_rate': 0.07242196012176288,\n",
        "               'optimizer': 'SGD'}\n",
        "\n",
        "hospitalbot_v2 = Sequential()\n",
        "hospitalbot_v2.add(Dense(10, input_shape=(len(training_data_tfidf[0]),)))\n",
        "hospitalbot_v2.add(Dense(8))\n",
        "hospitalbot_v2.add(Dense(8))\n",
        "hospitalbot_v2.add(Dense(6))\n",
        "hospitalbot_v2.add(Dense(len(training_data_tags_dummy_encoded[0]),\n",
        "                      activation=\"softmax\"))\n",
        "hospitalbot_v2.compile(optimizer=\"SGD\", loss=\"categorical_crossentropy\",\n",
        "                    metrics=\"accuracy\")\n",
        "\n",
        "hospitalbot_v2.fit(training_data_tfidf, training_data_tags_dummy_encoded,\n",
        "                epochs=15, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCAfIDdknQ5N",
        "outputId": "7cd9667e-73a6-4fb6-a8c8-824241b8b277"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "12/12 [==============================] - 1s 2ms/step - loss: 2.3388 - accuracy: 0.1158\n",
            "Epoch 2/15\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.3307 - accuracy: 0.1053\n",
            "Epoch 3/15\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.3231 - accuracy: 0.0947\n",
            "Epoch 4/15\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2.3157 - accuracy: 0.1158\n",
            "Epoch 5/15\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.3089 - accuracy: 0.1158\n",
            "Epoch 6/15\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2.3026 - accuracy: 0.1105\n",
            "Epoch 7/15\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.2960 - accuracy: 0.1263\n",
            "Epoch 8/15\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.2899 - accuracy: 0.1263\n",
            "Epoch 9/15\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.2847 - accuracy: 0.1421\n",
            "Epoch 10/15\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2792 - accuracy: 0.1316\n",
            "Epoch 11/15\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2.2727 - accuracy: 0.1316\n",
            "Epoch 12/15\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2.2670 - accuracy: 0.1211\n",
            "Epoch 13/15\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2.2613 - accuracy: 0.1316\n",
            "Epoch 14/15\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2.2550 - accuracy: 0.1474\n",
            "Epoch 15/15\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2.2491 - accuracy: 0.1684\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79a3d4801ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(hospitalbot_v2, \"HospitalBot_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdlBmQHUo07u",
        "outputId": "13eb0ad4-9108-45d4-cb83-3ace57e6971b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = load_model(\"HospitalBot_v2\")"
      ],
      "metadata": {
        "id": "mV90R1cuo6iy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our new chatbot, below are all of the new chat functionality. I did these before doing the Hyperparameter tuning haha but I moved it around now in the notebook to represent the optimal workflow that I should have implemented"
      ],
      "metadata": {
        "id": "jmGAFk9Vpt4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags = [item['tag'] for item in intents['intents']]\n",
        "contexts = [item['context'] for item in intents['intents']]\n",
        "tag_contexts = dict(zip(tags, contexts))\n",
        "tag_contexts"
      ],
      "metadata": {
        "id": "zxf8cJsKF5nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e208657d-7e75-4806-b683-447f198fc93f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'greeting': [''],\n",
              " 'goodbye': [''],\n",
              " 'thanks': [''],\n",
              " 'noanswer': [''],\n",
              " 'options': [''],\n",
              " 'adverse_drug': [''],\n",
              " 'blood_pressure': [''],\n",
              " 'blood_pressure_search': ['search_blood_pressure_by_patient_id'],\n",
              " 'search_blood_pressure_by_patient_id': [''],\n",
              " 'pharmacy_search': ['search_pharmacy_by_name'],\n",
              " 'search_pharmacy_by_name': [''],\n",
              " 'hospital_search': ['search_hospital_by_params'],\n",
              " 'search_hospital_by_params': ['search_hospital_by_type'],\n",
              " 'search_hospital_by_type': ['']}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use an updated predict_tag function that implements Intent\n",
        "# Confidence Threshold\n",
        "def predict_tag_v2(user_input, confidence_threshold=0.7):\n",
        "    user_input_tfidf = vectorizer.transform([user_input.lower()]).toarray()\n",
        "    #print(f\"user_input_tfidf: {user_input_tfidf}\") For debugging purposes\n",
        "\n",
        "    predicted_proba = hospitalbot.predict(user_input_tfidf)\n",
        "    #print(f\"predicted_proba: {predicted_proba}\") For debugging purposes\n",
        "\n",
        "    max_confidence = np.max(predicted_proba)\n",
        "\n",
        "    if max_confidence >= confidence_threshold:\n",
        "        encoded_label = [np.argmax(predicted_proba)]\n",
        "        predicted_tag = le.inverse_transform(encoded_label)[0]\n",
        "        #print(f\"predicted_tag: {predicted_tag}\") For debugging purposes\n",
        "        return predicted_tag\n",
        "    else:\n",
        "        # If confidence is below the threshold, return a special tag for\n",
        "        # unknown input\n",
        "        return \"unknown_input\"\n",
        "\n",
        "\n",
        "# We will also implement fuzzy matching so that it not only can pick up on\n",
        "# \"hospital\" and associate it with the \"hospital_search\" tag, but also any\n",
        "# slight variation or misspelling of it\n",
        "# def fuzzy_match(user_input, confidence_threshold=0.8):\n",
        "#     valid_matches = [\"blood\", \"pharmacy\", \"hospital\"]\n",
        "#     fuzzy_match = [valid_match for valid_match in valid_matches if fuzz.partial_ratio(user_input, valid_match) >= confidence_threshold]\n",
        "\n",
        "#     if fuzzy_match:\n",
        "#       return fuzzy_match[0]\n",
        "#     else:\n",
        "#       return \"unknown_input\"\n",
        "\n",
        "# Updated version where we define the order of valid matches and select\n",
        "# the one with the highest fuzzy match ratio. We also have an unknown\n",
        "# threshold so that the bot will still respond with a version of\n",
        "# \"I didn't understand that\" if we are typing in gibberish\n",
        "def fuzzy_match(user_input, confidence_threshold=0.8, unknown_threshold=70):\n",
        "    valid_matches = [\"hospital\", \"pharmacy\", \"blood\", \"blood pressure\",\n",
        "                     \"adverse drug\", \"hi\", \"hello\", \"help\"]\n",
        "\n",
        "    # Initialize variables to store the best match and its ratio\n",
        "    best_match = None\n",
        "    best_ratio = 0\n",
        "\n",
        "    for valid_match in valid_matches:\n",
        "        if valid_match.lower() in user_input.lower():  # Exact match for\n",
        "        #certain keywords\n",
        "            ratio = 100\n",
        "        else:\n",
        "            ratio = fuzz.partial_ratio(user_input, valid_match)\n",
        "\n",
        "        # Check if the current match has a higher ratio than the best match\n",
        "        if ratio >= confidence_threshold and ratio > best_ratio:\n",
        "            best_match = valid_match\n",
        "            best_ratio = ratio\n",
        "\n",
        "    if best_match and best_ratio >= unknown_threshold:\n",
        "        return best_match\n",
        "    else:\n",
        "        return \"unknown_input\""
      ],
      "metadata": {
        "id": "pXKpIybj5uBO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apart from just the tag, we will also consider the current context for\n",
        "# more context-aware responses and staying on topic\n",
        "default_messages = [\"I didn't catch that. Can you please try again?\",\n",
        "                    \"I'm sorry, I didn't understand that. Can you please provide more information?\",\n",
        "                    \"I'm not sure I understand. Can you help me understand you better?\"]\n",
        "\n",
        "def generate_response(input_tag, input_context):\n",
        "    if input_tag == \"unknown_input\":\n",
        "      # If the chatbot doesn't understand, it will display one of our\n",
        "      # variations of \"I don't understand\"\n",
        "        return random.choice(default_messages)\n",
        "    elif input_context is not None and len(input_context) > 2:\n",
        "        # If we have a context, use it to fetch responses\n",
        "        return random.choice(tag_responses.get(input_context, [\"\"]))\n",
        "    #elif len(input_tag) > 2:\n",
        "    else:\n",
        "        # If no context, use the tag to fetch responses\n",
        "        return random.choice(tag_responses.get(input_tag, [\"\"]))\n",
        "\n",
        "# Logic to update the current context based on the current tag\n",
        "# We return an empty string as a default value is the specified key\n",
        "# is not found\n",
        "def update_context(input_tag):\n",
        "    global current_context\n",
        "    new_context = tag_contexts.get(input_tag, [\"\"])[0]\n",
        "    current_context = new_context if len(new_context) > 2 else \"\""
      ],
      "metadata": {
        "id": "pSfOyHDYFq-G"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_letter_at_a_time(output_string:str):\n",
        "    for char in output_string:\n",
        "      print(char, end='')\n",
        "      sys.stdout.flush()\n",
        "      time.sleep(0.03)\n",
        "\n",
        "ending_message = \"Good luck! I will be here if you need me again!\"\n",
        "def end_chatbot():\n",
        "    time.sleep(2)\n",
        "    one_letter_at_a_time(\"•\\n•\\n•\\n•\\n\")\n",
        "    time.sleep(2)\n",
        "    one_letter_at_a_time(ending_message)\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "5-4dzoTAtPKO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_context = \"\"  # Initializing current context\n",
        "\n",
        "def start_chat_v2():\n",
        "    global current_context\n",
        "    print(\"---------------HospitalBot V2---------------\")\n",
        "    print(\"Ask any queries!\")\n",
        "    print(\"Type EXIT to quit\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"Type here: \")\n",
        "        if user_input == \"EXIT\":\n",
        "            time.sleep(1)\n",
        "            break\n",
        "        else:\n",
        "            if user_input:\n",
        "                # print(f\"Current context: {current_context}\")\n",
        "                # For debugging purposes\n",
        "\n",
        "                # If we have a current context, the tag is grabbed based on\n",
        "                # the context in the generate_response function\n",
        "                if current_context is not None and len(current_context) > 2:\n",
        "                    tag = current_context\n",
        "                else:\n",
        "                    # If it's the first time in the chat loop or if we do not have\n",
        "                    # a current context, we predict the tag based on user input,\n",
        "                    # after a fuzzy match check\n",
        "                    fuzzy_check = fuzzy_match(user_input.lower())\n",
        "                    tag = predict_tag_v2(fuzzy_check)\n",
        "\n",
        "                # We generate a response with both the current tag and\n",
        "                # context in mind (if we have one)\n",
        "                response = generate_response(tag, current_context)\n",
        "\n",
        "                # Update context based on the current tag\n",
        "                update_context(tag) # I had this as an assignment which\n",
        "                # returned None and it was driving me crazy for an hour\n",
        "                current_context = current_context\n",
        "                time.sleep(1)\n",
        "\n",
        "                # Below are the tags that end the conversation\n",
        "                # I tried a lot of solutions but I opted for this rather\n",
        "                # naïve looking one where we have branches of conversation\n",
        "                # based on the tag. This works because these context-specific\n",
        "                # tags have no patterns leading to them!\n",
        "                if tag == 'search_blood_pressure_by_patient_id':\n",
        "                  one_letter_at_a_time(f\"{response} #{user_input}\\n\")\n",
        "                  end_chatbot()\n",
        "                  break\n",
        "                elif tag == 'search_pharmacy_by_name':\n",
        "                  one_letter_at_a_time(f\"{response} for {user_input}\\n\")\n",
        "                  end_chatbot()\n",
        "                  break\n",
        "\n",
        "                # This one is a bit special in that it doesn't end the chatbot\n",
        "                # Instead it saves the Hospital name/location in order to\n",
        "                # display it in the next step of the context interaction\n",
        "                elif tag == 'search_hospital_by_params':\n",
        "                  hospital_param = user_input\n",
        "                  one_letter_at_a_time(response + \"\\n\")\n",
        "                  time.sleep(1)\n",
        "                elif tag == 'search_hospital_by_type':\n",
        "                  if 'hospital' in hospital_param.lower():\n",
        "                    one_letter_at_a_time(f\"{response} for {hospital_param}\\n\")\n",
        "                  else:\n",
        "                    one_letter_at_a_time(f\"{response} for {hospital_param} {user_input} Hospital\\n\")\n",
        "                  end_chatbot()\n",
        "                  break\n",
        "\n",
        "                else:\n",
        "                  one_letter_at_a_time(response + \"\\n\")\n",
        "                  time.sleep(1)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "start_chat_v2()"
      ],
      "metadata": {
        "id": "y2vTgTktC0Pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec96733e-5e48-4d1e-beb0-c126ae3f6f24"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------HospitalBot V2---------------\n",
            "Ask any queries!\n",
            "Type EXIT to quit\n",
            "\n",
            "Type here: oawuevsjdn\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "I'm not sure I understand. Can you help me understand you better?\n",
            "Type here: Duck\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "I'm not sure I understand. Can you help me understand you better?\n",
            "Type here: Hello\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Hi there, how can I help?\n",
            "Type here: hsptak\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "I'm sorry, I didn't understand that. Can you please provide more information?\n",
            "Type here: hosptak\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Please provide hospital name or location\n",
            "Type here: Stockholm City\n",
            "Please provide hospital type\n",
            "Type here: Regional\n",
            "Loading hospital details for Stockholm City Regional Hospital\n",
            "•\n",
            "•\n",
            "•\n",
            "•\n",
            "Good luck! I will be here if you need me again!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check accuracy!"
      ],
      "metadata": {
        "id": "gzeCocW4BQ3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    training_data_tfidf,\n",
        "    training_data_tags_dummy_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Defining early stopping callback to prevent overfitting by stopping the\n",
        "# training when the model's performance on a validation set stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3,\n",
        "                               restore_best_weights=True)\n",
        "\n",
        "# Training the model with early stopping implemented\n",
        "history = hospitalbot_v2.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,  # Increase the number of epochs or set it based on your training needs\n",
        "    batch_size=32,\n",
        "    verbose=1,\n",
        "    validation_split=0.1,  # Use a portion of the training data for validation\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "rjbJ805gtk4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c251a27-7cd1-4cf9-b65e-7e05b0dee099"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 2.2524 - accuracy: 0.1324 - val_loss: 2.2321 - val_accuracy: 0.1250\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 2.2494 - accuracy: 0.1324 - val_loss: 2.2302 - val_accuracy: 0.1250\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 2.2472 - accuracy: 0.1324 - val_loss: 2.2264 - val_accuracy: 0.1875\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 2.2438 - accuracy: 0.1618 - val_loss: 2.2238 - val_accuracy: 0.1875\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 2.2414 - accuracy: 0.1838 - val_loss: 2.2223 - val_accuracy: 0.1875\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 2.2384 - accuracy: 0.1838 - val_loss: 2.2210 - val_accuracy: 0.1875\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 2.2359 - accuracy: 0.1838 - val_loss: 2.2186 - val_accuracy: 0.1875\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 2.2332 - accuracy: 0.1838 - val_loss: 2.2163 - val_accuracy: 0.1875\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 2.2304 - accuracy: 0.1838 - val_loss: 2.2153 - val_accuracy: 0.1875\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 2.2280 - accuracy: 0.1838 - val_loss: 2.2131 - val_accuracy: 0.1875\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 2.2253 - accuracy: 0.1838 - val_loss: 2.2131 - val_accuracy: 0.1875\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 2.2224 - accuracy: 0.1838 - val_loss: 2.2120 - val_accuracy: 0.1875\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 2.2198 - accuracy: 0.1838 - val_loss: 2.2122 - val_accuracy: 0.1875\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 2.2179 - accuracy: 0.1912 - val_loss: 2.2100 - val_accuracy: 0.2500\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 2.2150 - accuracy: 0.2206 - val_loss: 2.2051 - val_accuracy: 0.1875\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 2.2115 - accuracy: 0.1838 - val_loss: 2.2018 - val_accuracy: 0.1875\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 2.2086 - accuracy: 0.1838 - val_loss: 2.2030 - val_accuracy: 0.2500\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 2.2054 - accuracy: 0.2279 - val_loss: 2.2006 - val_accuracy: 0.1875\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 2.2030 - accuracy: 0.2132 - val_loss: 2.1984 - val_accuracy: 0.1875\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 2.1998 - accuracy: 0.2206 - val_loss: 2.1959 - val_accuracy: 0.1875\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 2.1971 - accuracy: 0.2279 - val_loss: 2.1930 - val_accuracy: 0.1875\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 2.1944 - accuracy: 0.2132 - val_loss: 2.1916 - val_accuracy: 0.1875\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 2.1909 - accuracy: 0.2132 - val_loss: 2.1900 - val_accuracy: 0.2500\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 2.1892 - accuracy: 0.2206 - val_loss: 2.1868 - val_accuracy: 0.2500\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 2.1849 - accuracy: 0.2353 - val_loss: 2.1847 - val_accuracy: 0.2500\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 2.1822 - accuracy: 0.2353 - val_loss: 2.1804 - val_accuracy: 0.2500\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 2.1786 - accuracy: 0.2426 - val_loss: 2.1797 - val_accuracy: 0.2500\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 2.1758 - accuracy: 0.2500 - val_loss: 2.1776 - val_accuracy: 0.2500\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 2.1727 - accuracy: 0.2426 - val_loss: 2.1765 - val_accuracy: 0.2500\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 2.1695 - accuracy: 0.2500 - val_loss: 2.1741 - val_accuracy: 0.2500\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 2.1666 - accuracy: 0.2500 - val_loss: 2.1714 - val_accuracy: 0.2500\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 2.1632 - accuracy: 0.2500 - val_loss: 2.1694 - val_accuracy: 0.2500\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 2.1601 - accuracy: 0.2500 - val_loss: 2.1682 - val_accuracy: 0.2500\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 2.1561 - accuracy: 0.2500 - val_loss: 2.1663 - val_accuracy: 0.2500\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 2.1530 - accuracy: 0.2500 - val_loss: 2.1635 - val_accuracy: 0.2500\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 2.1499 - accuracy: 0.2500 - val_loss: 2.1611 - val_accuracy: 0.2500\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 2.1463 - accuracy: 0.2500 - val_loss: 2.1580 - val_accuracy: 0.2500\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 2.1424 - accuracy: 0.2500 - val_loss: 2.1543 - val_accuracy: 0.2500\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 2.1387 - accuracy: 0.2500 - val_loss: 2.1524 - val_accuracy: 0.2500\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 2.1350 - accuracy: 0.2500 - val_loss: 2.1512 - val_accuracy: 0.2500\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 2.1319 - accuracy: 0.2500 - val_loss: 2.1463 - val_accuracy: 0.1250\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 2.1274 - accuracy: 0.2500 - val_loss: 2.1462 - val_accuracy: 0.1250\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 2.1236 - accuracy: 0.2500 - val_loss: 2.1418 - val_accuracy: 0.1250\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 25ms/step - loss: 2.1196 - accuracy: 0.2500 - val_loss: 2.1401 - val_accuracy: 0.1250\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 2.1159 - accuracy: 0.2500 - val_loss: 2.1339 - val_accuracy: 0.1250\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 2.1110 - accuracy: 0.2500 - val_loss: 2.1302 - val_accuracy: 0.1250\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 2.1071 - accuracy: 0.2500 - val_loss: 2.1273 - val_accuracy: 0.1250\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 2.1027 - accuracy: 0.2647 - val_loss: 2.1201 - val_accuracy: 0.1250\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 2.0975 - accuracy: 0.2500 - val_loss: 2.1131 - val_accuracy: 0.1250\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 2.0921 - accuracy: 0.2500 - val_loss: 2.1110 - val_accuracy: 0.1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "test_metrics = hospitalbot_v2.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Displaying the test accuracy and other metrics\n",
        "print(f\"Test Accuracy: {test_metrics[1]*100:.2f}%\")\n",
        "print(f\"Test Loss: {test_metrics[0]}\")\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = hospitalbot_v2.predict(X_test)\n",
        "\n",
        "# Converting predictions to class labels\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_test_classes = y_test.argmax(axis=1)\n",
        "\n",
        "# Displaying classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))"
      ],
      "metadata": {
        "id": "A_fgEwr3ttAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8696ff9-2919-4cf1-c5c3-ed31518772f4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 18.42%\n",
            "Test Loss: 2.1479597091674805\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.00      0.00      0.00         5\n",
            "           2       0.00      0.00      0.00         2\n",
            "           3       0.00      0.00      0.00         3\n",
            "           4       0.14      1.00      0.25         4\n",
            "           5       0.25      0.33      0.29         3\n",
            "           6       1.00      0.20      0.33        10\n",
            "           7       0.00      0.00      0.00         3\n",
            "           9       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.18        38\n",
            "   macro avg       0.15      0.17      0.10        38\n",
            "weighted avg       0.30      0.18      0.14        38\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the model actually performs significantly worse after the Hyperparameter tuning, hence why I now retroactively have chosen to run with the first model for the chatbot. I tried running the chat loop with the V2 DNN and it couldn't understand anything, it couldn't pick up a single tag.<br>\n",
        "Why does it perform worse after the Hyperparameter tuning? Let's explore a few reasons why this might be the case:\n",
        "*   The hyperparameter search space might have been too broad, and random search might have explored combinations that do not work well together.\n",
        "*   Since the dataset is rather small (and I have chosen to not add anything to the dataset that we are given), hyperparameter tuning might lead to models that are too complex and perform poorly on new data.\n",
        "*   The metric used for hyperparameter tuning might not have aligned with the actual goal of the application.\n",
        "\n",
        "Still, I made a rather functioning chatbot with the first DNN in combination with functionality for the chat loop such as context handling and fuzzy matching, and I am satisfied with the results! It doesn't understand \"hsptl\" and it doesn't understand \"hostpl\", but it does understand \"hosptl\". That is a fine line of understanding that I am okay with<br><br>\n",
        "Further improvements that I stumbled upon but have chosen not to implement include:\n",
        "*   Natural Language Understanding (NLU): We could incorporate pre-trained language models like BERT.\n",
        "*   Logging and Analytics: We could implement logging to capture all user queries entered, predicted intents, and bot responses. This can provide valuable insights into user interactions and areas for improvement.\n",
        "*   If wanted to actually deploy the chatbot, we would have to ensure that it can handle concurrent users and is scalable. We would consider deploying it on a platform that supports the desired level of usage.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "APrjfnbkcOTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a dump and a code graveyard of all old and brancing code snippets from endeavors of trying to make things like the context handling work. A lot of it is written by ChatGPT by having a collaborative conversation with it sharing my vision of my output. At one point it suggested a Dictionary to track context-specific flags and at that point I tapped out and went to bed. I woke up the next morning and fixed it on my own within the first 20 minutes of coding! (I cannot I prove it but I swear I am not lying when I say that haha)"
      ],
      "metadata": {
        "id": "zAnex02Dt56o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # def generate_response(input_tag, input_context):\n",
        "# #   global current_context\n",
        "# #   # If there is no current context, we simply grab a response from the\n",
        "# #   # tag dictionary and return it. Similar to the first version of the\n",
        "# #   # chatbot; we can even copy the cope from it\n",
        "# #   if len(input_context) < 2:\n",
        "# #     return random.choice(tag_responses[input_tag])\n",
        "\n",
        "# #   # If there is current context, the chatbot should behave differently and\n",
        "# #   # more aware.\n",
        "# #   else:\n",
        "# #     new_tag = tag_contexts[input_tag]\n",
        "# #     return random.choice(tag_responses[new_tag])\n",
        "\n",
        "# def update_context(input_tag):\n",
        "#     global current_context\n",
        "#     new_context = tag_contexts.get(input_tag, [\"\"])[0]\n",
        "#     current_context = new_context if new_context is not None and len(new_context) > 2 else \"\"\n",
        "#     print(f\"Updated context to: {current_context}\")\n",
        "\n",
        "# def update_context(input_tag):\n",
        "#     global current_context\n",
        "#     global end_of_interaction\n",
        "\n",
        "#     new_context = tag_contexts.get(input_tag, [\"\"])[0]\n",
        "#     current_context = new_context if len(new_context) > 2 else \"\"\n",
        "\n",
        "#     print(f\"New context before update: {new_context}\")  # Add this line for debugging\n",
        "#     # Set end_of_interaction to True if the new_context is not empty\n",
        "#     end_of_interaction = bool(new_context)\n",
        "#     print(f\"Updated context to: {current_context}\")  # Add this line for debugging\n",
        "#     print(f\"End of interaction: {end_of_interaction}\")  # Add this line for debugging\n",
        "\n",
        "# def generate_response(input_tag, input_context):\n",
        "#     global end_of_interaction\n",
        "\n",
        "#     # Logic to generate responses based on the tag and current context\n",
        "#     # Use the intents JSON to determine appropriate responses\n",
        "#     if input_context is not None and len(input_context) > 2:\n",
        "#         # If we have a context, use it to fetch responses.\n",
        "#         response = random.choice(tag_responses.get(input_context, [\"\"]))\n",
        "#         # Switch the global end_of_interaction boolean to True\n",
        "#         end_of_interaction = True\n",
        "#         return response\n",
        "#     elif len(input_tag) > 2:\n",
        "#         # If no context, use the tag to fetch responses\n",
        "#         return random.choice(tag_responses.get(input_tag, [\"\"]))\n",
        "#     else:\n",
        "#         # If nothing works, we chatbot will simply say it doesn't\n",
        "#         # understand\n",
        "#         return \"Sorry, I couldn't understand that.\"\n",
        "\n",
        "# def generate_response(input_tag, input_context):\n",
        "#     # Define context-specific actions\n",
        "#     context_actions = {\n",
        "#         \"search_blood_pressure_by_patient_id\": handle_blood_pressure_by_patient_id,\n",
        "#         # Add other context-action mappings as needed\n",
        "#     }\n",
        "\n",
        "#     # Check if the input_context has a corresponding action\n",
        "#     if input_context in context_actions:\n",
        "#         # Execute the context-specific action and get the response\n",
        "#         response = context_actions[input_context](input_tag)\n",
        "#         return response\n",
        "#     elif len(input_tag) > 2:\n",
        "#         # If no context, use the tag to fetch responses\n",
        "#         return random.choice(tag_responses.get(input_tag, [\"\"]))\n",
        "#     else:\n",
        "#         # If nothing works, the chatbot will simply say it doesn't understand\n",
        "#         return \"Sorry, I couldn't understand that.\"\n",
        "\n",
        "# # Dictionary to track context-specific flags\n",
        "# context_flags = {\n",
        "#     \"search_blood_pressure_by_patient_id\": False,\n",
        "#     # Add other contexts as needed\n",
        "# }\n",
        "\n",
        "# # Example context-specific action for \"search_blood_pressure_by_patient_id\"\n",
        "# def handle_blood_pressure_by_patient_id(input_tag):\n",
        "#     global context_flags\n",
        "\n",
        "#     # Implement the specific logic for this context\n",
        "#     response = random.choice(tag_responses.get(input_context, [\"\"]))\n",
        "\n",
        "#     # Check if the response contains \"Please provide Patient ID\" and set the flag accordingly\n",
        "#     context_flags[input_context] = \"Please provide Patient ID\" in response\n",
        "\n",
        "#     return response\n",
        "\n",
        "\n",
        "\n",
        "# def update_context(input_tag):\n",
        "#     global current_context\n",
        "#     global end_of_interaction\n",
        "\n",
        "#     new_context = tag_contexts.get(input_tag, [\"\"])[0]\n",
        "#     current_context = new_context if len(new_context) > 2 else \"\"\n",
        "\n",
        "#     # Set end_of_interaction to True if the current_context indicates the end\n",
        "#     end_of_interaction = \"Good luck!\" in current_context\n",
        "\n",
        "\n",
        "# def start_chat_v2():\n",
        "#     global current_context\n",
        "#     global end_of_interaction\n",
        "\n",
        "#     print(\"---------------HospitalBot V2---------------\")\n",
        "#     print(\"Ask any queries!\")\n",
        "#     print(\"Type EXIT to quit\\n\")\n",
        "\n",
        "#     while True:\n",
        "#       user_input = input(\"Enter here: \")\n",
        "\n",
        "#       if user_input == \"EXIT\":\n",
        "#           time.sleep(1)\n",
        "#           break\n",
        "\n",
        "#       if user_input:\n",
        "#           print(f\"Current context: {current_context}\")\n",
        "#           print(f\"End of interaction: {end_of_interaction}\")\n",
        "\n",
        "#           if len(current_context) < 2:\n",
        "#               tag = predict_tag(user_input)\n",
        "#               response = generate_response(tag, current_context)\n",
        "#               print(response)\n",
        "#               current_context = update_context(tag)\n",
        "\n",
        "#               # Check if we are waiting for specific information based on the context\n",
        "#               if current_context is not None and not context_flags.get(current_context, False):\n",
        "#                   continue  # Continue the loop to prompt for information\n",
        "\n",
        "#               # Check if the current context is empty and if end_of_interaction is True\n",
        "#               if not current_context and end_of_interaction:\n",
        "#                   print(\"Good luck! I will be here if you need me again\")\n",
        "#                   time.sleep(1)\n",
        "#                   print(\"Breaking out of the loop\")\n",
        "#                   break\n",
        "#           else:\n",
        "#               pass"
      ],
      "metadata": {
        "id": "xbz4dS3wuVHn"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}